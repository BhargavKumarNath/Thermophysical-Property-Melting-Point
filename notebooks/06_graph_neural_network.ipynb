{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "232b1840",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pickle\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "from rdkit import Chem\n",
    "from rdkit import RDLogger \n",
    "RDLogger.DisableLog('rdApp.*')\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import Linear, BatchNorm1d, Module, Sequential\n",
    "from torch_geometric.data import Data, Dataset\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.nn import GCNConv, global_mean_pool\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eb95391",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Step 1: Defining the SMILES to Graph Conversion (with Scaling and Correct Paths) ---\n"
     ]
    }
   ],
   "source": [
    "print(\"--- Step 1: Defining the SMILES to Graph Conversion (with Scaling and Correct Paths) ---\")\n",
    "\n",
    "def smiles_to_graph(smiles_string, y_val=0):\n",
    "    mol = Chem.MolFromSmiles(smiles_string)\n",
    "    if mol is None: return None\n",
    "    atom_features_list = []\n",
    "    for atom in mol.GetAtoms():\n",
    "        atom_features_list.append([\n",
    "            atom.GetAtomicNum(), atom.GetFormalCharge(), atom.GetHybridization(),\n",
    "            atom.GetIsAromatic(), atom.GetTotalNumHs(), atom.GetTotalValence(),\n",
    "        ])\n",
    "    x = torch.tensor(atom_features_list, dtype=torch.float)\n",
    "    edge_indices, edge_attrs = [], []\n",
    "    for bond in mol.GetBonds():\n",
    "        i, j = bond.GetBeginAtomIdx(), bond.GetEndAtomIdx()\n",
    "        bond_type = bond.GetBondTypeAsDouble()\n",
    "        edge_indices.extend([(i, j), (j, i)])\n",
    "        edge_attrs.extend([[bond_type], [bond_type]])\n",
    "    edge_index = torch.tensor(edge_indices, dtype=torch.long).t().contiguous()\n",
    "    edge_attr = torch.tensor(edge_attrs, dtype=torch.float)\n",
    "    data = Data(x=x, edge_index=edge_index, edge_attr=edge_attr, y=torch.tensor([y_val], dtype=torch.float))\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f2203b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MeltingPointDataset(Dataset):\n",
    "    def __init__(self, root, filename, original_data_path, test=False, scaler=None):\n",
    "        self.filename = filename\n",
    "        self.original_data_path = original_data_path\n",
    "        self.test = test\n",
    "        self.scaler = scaler\n",
    "        super(MeltingPointDataset, self).__init__(root)\n",
    "        \n",
    "    @property\n",
    "    def raw_file_names(self):\n",
    "        return self.filename\n",
    "\n",
    "    @property\n",
    "    def processed_file_names(self):\n",
    "        return f'{\"test\" if self.test else \"train\"}_data.pt'\n",
    "\n",
    "    def download(self):\n",
    "        source_path = os.path.join(self.original_data_path, self.filename)\n",
    "        dest_path = os.path.join(self.raw_dir, self.filename)\n",
    "        if not os.path.exists(dest_path):\n",
    "            print(f\"Copying {source_path} to {dest_path}\")\n",
    "            shutil.copy(source_path, dest_path)\n",
    "\n",
    "    def process(self):\n",
    "        self.data = pd.read_csv(self.raw_paths[0])\n",
    "        \n",
    "        if not self.test and self.scaler is None:\n",
    "            all_node_features = []\n",
    "            for smiles in tqdm(self.data['SMILES'], desc=\"Fitting Scaler\"):\n",
    "                mol = Chem.MolFromSmiles(smiles)\n",
    "                if mol:\n",
    "                    for atom in mol.GetAtoms():\n",
    "                        all_node_features.append([\n",
    "                            atom.GetAtomicNum(), atom.GetFormalCharge(), atom.GetHybridization(),\n",
    "                            atom.GetIsAromatic(), atom.GetTotalNumHs(), atom.GetTotalValence(),\n",
    "                        ])\n",
    "            self.scaler = StandardScaler()\n",
    "            self.scaler.fit(all_node_features)\n",
    "            os.makedirs('../models', exist_ok=True)\n",
    "            with open('../models/gnn_scaler.pkl', 'wb') as f:\n",
    "                pickle.dump(self.scaler, f)\n",
    "\n",
    "        graphs = []\n",
    "        for idx, row in tqdm(self.data.iterrows(), total=self.data.shape[0], desc=\"Processing SMILES\"):\n",
    "            y_val = np.log(row['Tm']) if not self.test else 0\n",
    "            graph = smiles_to_graph(row['SMILES'], y_val)\n",
    "            if graph is not None:\n",
    "                graph.x = torch.tensor(self.scaler.transform(graph.x), dtype=torch.float)\n",
    "                graphs.append(graph)\n",
    "        \n",
    "        torch.save(graphs, self.processed_paths[0])\n",
    "\n",
    "    def len(self):\n",
    "        if not hasattr(self, 'graphs'):\n",
    "             self.graphs = torch.load(self.processed_paths[0], weights_only=False)\n",
    "        return len(self.graphs)\n",
    "\n",
    "    def get(self, idx):\n",
    "        if not hasattr(self, 'graphs'):\n",
    "             self.graphs = torch.load(self.processed_paths[0], weights_only=False)\n",
    "        return self.graphs[idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f6eb8f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instantiating and processing datasets...\n",
      "Copying ../data/raw\\train.csv to ..\\data\\processed\\gnn_v4\\raw\\train.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b1a452132f54e1698b9a84e2105d6d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fitting Scaler:   0%|          | 0/2662 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c419426e9cea42eb8a70754a3fabdab2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing SMILES:   0%|          | 0/2662 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying ../data/raw\\test.csv to ..\\data\\processed\\gnn_v4\\raw\\test.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Done!\n",
      "Processing...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e8c2ebb506fc420d89a3f2988e3c4976",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing SMILES:   0%|          | 0/666 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Done!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datasets created. Number of training graphs: 2662\n",
      "Number of testing graphs: 666\n",
      "Number of node features: 6\n"
     ]
    }
   ],
   "source": [
    "print(\"Instantiating and processing datasets...\")\n",
    "ORIGINAL_RAW_PATH = '../data/raw'\n",
    "\n",
    "train_dataset = MeltingPointDataset(\n",
    "    root='../data/processed/gnn_v4', \n",
    "    filename='train.csv', \n",
    "    original_data_path=ORIGINAL_RAW_PATH\n",
    ")\n",
    "with open('../models/gnn_scaler.pkl', 'rb') as f:\n",
    "    scaler = pickle.load(f)\n",
    "test_dataset = MeltingPointDataset(\n",
    "    root='../data/processed/gnn_v4', \n",
    "    filename='test.csv', \n",
    "    original_data_path=ORIGINAL_RAW_PATH,\n",
    "    test=True, \n",
    "    scaler=scaler\n",
    ")\n",
    "print(f\"Datasets created. Number of training graphs: {len(train_dataset)}\")\n",
    "print(f\"Number of testing graphs: {len(test_dataset)}\")\n",
    "print(f\"Number of node features: {train_dataset.num_node_features}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58dc2f6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Step 2: Defining a more Regularized GNN Model ---\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- Step 2: Defining a more Regularized GNN Model ---\")\n",
    "\n",
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self, num_node_features, hidden_channels=128): \n",
    "        super(GCN, self).__init__()\n",
    "        torch.manual_seed(42)\n",
    "        self.conv1 = GCNConv(num_node_features, hidden_channels)\n",
    "        self.bn1 = BatchNorm1d(hidden_channels)\n",
    "        self.conv2 = GCNConv(hidden_channels, hidden_channels * 2)\n",
    "        self.bn2 = BatchNorm1d(hidden_channels * 2)\n",
    "        self.mlp = Sequential(\n",
    "            Linear(hidden_channels * 2, hidden_channels),\n",
    "            torch.nn.ReLU(),\n",
    "            Linear(hidden_channels, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index, batch = data.x, data.edge_index, data.batch\n",
    "        x = self.conv1(x, edge_index).relu()\n",
    "        x = self.bn1(x)\n",
    "        x = self.conv2(x, edge_index).relu()\n",
    "        x = self.bn2(x)\n",
    "        x = global_mean_pool(x, batch)\n",
    "        x = F.dropout(x, p=0.4, training=self.training) \n",
    "        x = self.mlp(x)\n",
    "        return x.squeeze()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4830b37b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Step 3: Setting up the STABILIZED Training Pipeline ---\n",
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# 3. Stabilized Training and Evaluation Pipeline\n",
    "print(\"\\n--- Step 3: Setting up the STABILIZED Training Pipeline ---\")\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "torch.manual_seed(42)\n",
    "shuffled_dataset = train_dataset.shuffle()\n",
    "train_size = int(0.85 * len(shuffled_dataset))\n",
    "train_data, val_data = shuffled_dataset[:train_size], shuffled_dataset[train_size:]\n",
    "train_loader = DataLoader(train_data, batch_size=64, shuffle=True)\n",
    "val_loader = DataLoader(val_data, batch_size=64, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "model = GCN(num_node_features=train_dataset.num_node_features).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=5e-4) \n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.7, patience=5, min_lr=1e-6)\n",
    "criterion = torch.nn.L1Loss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ed5cc61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Step 4: Starting STABILIZED GNN Model Training ---\n",
      "Epoch: 001, Loss: 4.0308, Val MAE: 408.7085, Best Val MAE: 408.7085, LR: 0.000500\n",
      "Epoch: 002, Loss: 1.2667, Val MAE: 823.5269, Best Val MAE: 408.7085, LR: 0.000500\n",
      "Epoch: 003, Loss: 1.0187, Val MAE: 159.0577, Best Val MAE: 159.0577, LR: 0.000500\n",
      "Epoch: 004, Loss: 1.0274, Val MAE: 128.4084, Best Val MAE: 128.4084, LR: 0.000500\n",
      "Epoch: 005, Loss: 0.8890, Val MAE: 135.3870, Best Val MAE: 128.4084, LR: 0.000500\n",
      "Epoch: 006, Loss: 0.8749, Val MAE: 113.2082, Best Val MAE: 113.2082, LR: 0.000500\n",
      "Epoch: 007, Loss: 0.8564, Val MAE: 157.2214, Best Val MAE: 113.2082, LR: 0.000500\n",
      "Epoch: 008, Loss: 0.7845, Val MAE: 94.6739, Best Val MAE: 94.6739, LR: 0.000500\n",
      "Epoch: 009, Loss: 0.7563, Val MAE: 95.7280, Best Val MAE: 94.6739, LR: 0.000500\n",
      "Epoch: 010, Loss: 0.7496, Val MAE: 112.2653, Best Val MAE: 94.6739, LR: 0.000500\n",
      "Epoch: 011, Loss: 0.6744, Val MAE: 120.5371, Best Val MAE: 94.6739, LR: 0.000500\n",
      "Epoch: 012, Loss: 0.6676, Val MAE: 100.6247, Best Val MAE: 94.6739, LR: 0.000500\n",
      "Epoch: 013, Loss: 0.6577, Val MAE: 105.0851, Best Val MAE: 94.6739, LR: 0.000500\n",
      "Epoch: 014, Loss: 0.6070, Val MAE: 94.7221, Best Val MAE: 94.6739, LR: 0.000350\n",
      "Epoch: 015, Loss: 0.6054, Val MAE: 93.4198, Best Val MAE: 93.4198, LR: 0.000350\n",
      "Epoch: 016, Loss: 0.5994, Val MAE: 78.1818, Best Val MAE: 78.1818, LR: 0.000350\n",
      "Epoch: 017, Loss: 0.6085, Val MAE: 72.5843, Best Val MAE: 72.5843, LR: 0.000350\n",
      "Epoch: 018, Loss: 0.5656, Val MAE: 98.9937, Best Val MAE: 72.5843, LR: 0.000350\n",
      "Epoch: 019, Loss: 0.5725, Val MAE: 110.3780, Best Val MAE: 72.5843, LR: 0.000350\n",
      "Epoch: 020, Loss: 0.5615, Val MAE: 100.8627, Best Val MAE: 72.5843, LR: 0.000350\n",
      "Epoch: 021, Loss: 0.5815, Val MAE: 84.1875, Best Val MAE: 72.5843, LR: 0.000350\n",
      "Epoch: 022, Loss: 0.5720, Val MAE: 83.0169, Best Val MAE: 72.5843, LR: 0.000350\n",
      "Epoch: 023, Loss: 0.5444, Val MAE: 83.4044, Best Val MAE: 72.5843, LR: 0.000245\n",
      "Epoch: 024, Loss: 0.5215, Val MAE: 80.7018, Best Val MAE: 72.5843, LR: 0.000245\n",
      "Epoch: 025, Loss: 0.5081, Val MAE: 75.4492, Best Val MAE: 72.5843, LR: 0.000245\n",
      "Epoch: 026, Loss: 0.4790, Val MAE: 74.9894, Best Val MAE: 72.5843, LR: 0.000245\n",
      "Epoch: 027, Loss: 0.4945, Val MAE: 76.2486, Best Val MAE: 72.5843, LR: 0.000245\n",
      "Epoch: 028, Loss: 0.4951, Val MAE: 74.5193, Best Val MAE: 72.5843, LR: 0.000245\n",
      "Epoch: 029, Loss: 0.5066, Val MAE: 81.4450, Best Val MAE: 72.5843, LR: 0.000171\n",
      "Epoch: 030, Loss: 0.4763, Val MAE: 73.5058, Best Val MAE: 72.5843, LR: 0.000171\n",
      "Epoch: 031, Loss: 0.4669, Val MAE: 96.9878, Best Val MAE: 72.5843, LR: 0.000171\n",
      "Epoch: 032, Loss: 0.4494, Val MAE: 71.2984, Best Val MAE: 71.2984, LR: 0.000171\n",
      "Epoch: 033, Loss: 0.4682, Val MAE: 76.4516, Best Val MAE: 71.2984, LR: 0.000171\n",
      "Epoch: 034, Loss: 0.4791, Val MAE: 80.6327, Best Val MAE: 71.2984, LR: 0.000171\n",
      "Epoch: 035, Loss: 0.4654, Val MAE: 67.6658, Best Val MAE: 67.6658, LR: 0.000171\n",
      "Epoch: 036, Loss: 0.4387, Val MAE: 71.4612, Best Val MAE: 67.6658, LR: 0.000171\n",
      "Epoch: 037, Loss: 0.4524, Val MAE: 92.2189, Best Val MAE: 67.6658, LR: 0.000171\n",
      "Epoch: 038, Loss: 0.4424, Val MAE: 79.3145, Best Val MAE: 67.6658, LR: 0.000171\n",
      "Epoch: 039, Loss: 0.4282, Val MAE: 79.8611, Best Val MAE: 67.6658, LR: 0.000171\n",
      "Epoch: 040, Loss: 0.4479, Val MAE: 69.0206, Best Val MAE: 67.6658, LR: 0.000171\n",
      "Epoch: 041, Loss: 0.4629, Val MAE: 75.3109, Best Val MAE: 67.6658, LR: 0.000120\n",
      "Epoch: 042, Loss: 0.4517, Val MAE: 62.3036, Best Val MAE: 62.3036, LR: 0.000120\n",
      "Epoch: 043, Loss: 0.4323, Val MAE: 64.2185, Best Val MAE: 62.3036, LR: 0.000120\n",
      "Epoch: 044, Loss: 0.4241, Val MAE: 73.6222, Best Val MAE: 62.3036, LR: 0.000120\n",
      "Epoch: 045, Loss: 0.4215, Val MAE: 79.4593, Best Val MAE: 62.3036, LR: 0.000120\n",
      "Epoch: 046, Loss: 0.4207, Val MAE: 71.7846, Best Val MAE: 62.3036, LR: 0.000120\n",
      "Epoch: 047, Loss: 0.4225, Val MAE: 94.4701, Best Val MAE: 62.3036, LR: 0.000120\n",
      "Epoch: 048, Loss: 0.4275, Val MAE: 76.7661, Best Val MAE: 62.3036, LR: 0.000084\n",
      "Epoch: 049, Loss: 0.4197, Val MAE: 61.3821, Best Val MAE: 61.3821, LR: 0.000084\n",
      "Epoch: 050, Loss: 0.4217, Val MAE: 79.7264, Best Val MAE: 61.3821, LR: 0.000084\n",
      "Epoch: 051, Loss: 0.4123, Val MAE: 71.0092, Best Val MAE: 61.3821, LR: 0.000084\n",
      "Epoch: 052, Loss: 0.4204, Val MAE: 65.5143, Best Val MAE: 61.3821, LR: 0.000084\n",
      "Epoch: 053, Loss: 0.4185, Val MAE: 68.3929, Best Val MAE: 61.3821, LR: 0.000084\n",
      "Epoch: 054, Loss: 0.4150, Val MAE: 67.6517, Best Val MAE: 61.3821, LR: 0.000084\n",
      "Epoch: 055, Loss: 0.4076, Val MAE: 61.0223, Best Val MAE: 61.0223, LR: 0.000084\n",
      "Epoch: 056, Loss: 0.4031, Val MAE: 66.5233, Best Val MAE: 61.0223, LR: 0.000084\n",
      "Epoch: 057, Loss: 0.4144, Val MAE: 65.4104, Best Val MAE: 61.0223, LR: 0.000084\n",
      "Epoch: 058, Loss: 0.4061, Val MAE: 60.7010, Best Val MAE: 60.7010, LR: 0.000084\n",
      "Epoch: 059, Loss: 0.4071, Val MAE: 60.2851, Best Val MAE: 60.2851, LR: 0.000084\n",
      "Epoch: 060, Loss: 0.4029, Val MAE: 68.8591, Best Val MAE: 60.2851, LR: 0.000084\n",
      "Epoch: 061, Loss: 0.4004, Val MAE: 67.5837, Best Val MAE: 60.2851, LR: 0.000084\n",
      "Epoch: 062, Loss: 0.3854, Val MAE: 75.1872, Best Val MAE: 60.2851, LR: 0.000084\n",
      "Epoch: 063, Loss: 0.4009, Val MAE: 62.7963, Best Val MAE: 60.2851, LR: 0.000084\n",
      "Epoch: 064, Loss: 0.4016, Val MAE: 62.7453, Best Val MAE: 60.2851, LR: 0.000084\n",
      "Epoch: 065, Loss: 0.3988, Val MAE: 67.9203, Best Val MAE: 60.2851, LR: 0.000059\n",
      "Epoch: 066, Loss: 0.3918, Val MAE: 58.4494, Best Val MAE: 58.4494, LR: 0.000059\n",
      "Epoch: 067, Loss: 0.3925, Val MAE: 60.6427, Best Val MAE: 58.4494, LR: 0.000059\n",
      "Epoch: 068, Loss: 0.3946, Val MAE: 67.6259, Best Val MAE: 58.4494, LR: 0.000059\n",
      "Epoch: 069, Loss: 0.3847, Val MAE: 64.2151, Best Val MAE: 58.4494, LR: 0.000059\n",
      "Epoch: 070, Loss: 0.3931, Val MAE: 66.1166, Best Val MAE: 58.4494, LR: 0.000059\n",
      "Epoch: 071, Loss: 0.3951, Val MAE: 65.1818, Best Val MAE: 58.4494, LR: 0.000059\n",
      "Epoch: 072, Loss: 0.3725, Val MAE: 66.0003, Best Val MAE: 58.4494, LR: 0.000041\n",
      "Epoch: 073, Loss: 0.3736, Val MAE: 67.9337, Best Val MAE: 58.4494, LR: 0.000041\n",
      "Epoch: 074, Loss: 0.3778, Val MAE: 61.1714, Best Val MAE: 58.4494, LR: 0.000041\n",
      "Epoch: 075, Loss: 0.3877, Val MAE: 64.7615, Best Val MAE: 58.4494, LR: 0.000041\n",
      "Epoch: 076, Loss: 0.3936, Val MAE: 57.0953, Best Val MAE: 57.0953, LR: 0.000041\n",
      "Epoch: 077, Loss: 0.3813, Val MAE: 67.0871, Best Val MAE: 57.0953, LR: 0.000041\n",
      "Epoch: 078, Loss: 0.3735, Val MAE: 65.4718, Best Val MAE: 57.0953, LR: 0.000041\n",
      "Epoch: 079, Loss: 0.3777, Val MAE: 60.8951, Best Val MAE: 57.0953, LR: 0.000041\n",
      "Epoch: 080, Loss: 0.3906, Val MAE: 61.2034, Best Val MAE: 57.0953, LR: 0.000041\n",
      "Epoch: 081, Loss: 0.3917, Val MAE: 56.5502, Best Val MAE: 56.5502, LR: 0.000041\n",
      "Epoch: 082, Loss: 0.3750, Val MAE: 60.9745, Best Val MAE: 56.5502, LR: 0.000041\n",
      "Epoch: 083, Loss: 0.3706, Val MAE: 61.9990, Best Val MAE: 56.5502, LR: 0.000041\n",
      "Epoch: 084, Loss: 0.3809, Val MAE: 61.0211, Best Val MAE: 56.5502, LR: 0.000041\n",
      "Epoch: 085, Loss: 0.3922, Val MAE: 61.2971, Best Val MAE: 56.5502, LR: 0.000041\n",
      "Epoch: 086, Loss: 0.3881, Val MAE: 64.6961, Best Val MAE: 56.5502, LR: 0.000041\n",
      "Epoch: 087, Loss: 0.3912, Val MAE: 59.0032, Best Val MAE: 56.5502, LR: 0.000029\n",
      "Epoch: 088, Loss: 0.3746, Val MAE: 68.9259, Best Val MAE: 56.5502, LR: 0.000029\n",
      "Epoch: 089, Loss: 0.3786, Val MAE: 58.2185, Best Val MAE: 56.5502, LR: 0.000029\n",
      "Epoch: 090, Loss: 0.3807, Val MAE: 60.0735, Best Val MAE: 56.5502, LR: 0.000029\n",
      "Epoch: 091, Loss: 0.3624, Val MAE: 60.4296, Best Val MAE: 56.5502, LR: 0.000029\n",
      "Epoch: 092, Loss: 0.3812, Val MAE: 63.0841, Best Val MAE: 56.5502, LR: 0.000029\n",
      "Epoch: 093, Loss: 0.3791, Val MAE: 60.3970, Best Val MAE: 56.5502, LR: 0.000020\n",
      "Epoch: 094, Loss: 0.3683, Val MAE: 59.2063, Best Val MAE: 56.5502, LR: 0.000020\n",
      "Epoch: 095, Loss: 0.3843, Val MAE: 64.1426, Best Val MAE: 56.5502, LR: 0.000020\n",
      "Epoch: 096, Loss: 0.3728, Val MAE: 67.2459, Best Val MAE: 56.5502, LR: 0.000020\n",
      "Epoch: 097, Loss: 0.3679, Val MAE: 57.8782, Best Val MAE: 56.5502, LR: 0.000020\n",
      "Epoch: 098, Loss: 0.3772, Val MAE: 57.7612, Best Val MAE: 56.5502, LR: 0.000020\n",
      "Epoch: 099, Loss: 0.3666, Val MAE: 62.7757, Best Val MAE: 56.5502, LR: 0.000014\n",
      "Epoch: 100, Loss: 0.3654, Val MAE: 63.1015, Best Val MAE: 56.5502, LR: 0.000014\n",
      "Epoch: 101, Loss: 0.3712, Val MAE: 58.4616, Best Val MAE: 56.5502, LR: 0.000014\n",
      "Early stopping at epoch 101.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "def train():\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for data in train_loader:\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        out = model(data)\n",
    "        loss = criterion(out, data.y)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0) \n",
    "        optimizer.step()\n",
    "        total_loss += loss.item() * data.num_graphs\n",
    "    return total_loss / len(train_loader.dataset)\n",
    "\n",
    "def evaluate(loader):\n",
    "    model.eval()\n",
    "    all_preds, all_reals = [], []\n",
    "    with torch.no_grad():\n",
    "        for data in loader:\n",
    "            data = data.to(device)\n",
    "            out = model(data)\n",
    "            all_preds.extend(np.exp(out.cpu().numpy()))\n",
    "            all_reals.extend(np.exp(data.y.cpu().numpy()))\n",
    "    return mean_absolute_error(all_reals, all_preds)\n",
    "\n",
    "# 4. Run the Stabilized Training\n",
    "print(\"\\n--- Step 4: Starting STABILIZED GNN Model Training ---\")\n",
    "\n",
    "best_val_mae = float('inf')\n",
    "patience_counter = 0\n",
    "patience = 20\n",
    "\n",
    "for epoch in range(1, 201):\n",
    "    loss = train()\n",
    "    val_mae = evaluate(val_loader)\n",
    "    scheduler.step(val_mae)\n",
    "    \n",
    "    if val_mae < best_val_mae:\n",
    "        best_val_mae = val_mae\n",
    "        torch.save(model.state_dict(), 'best_gnn_model_v2.pt')\n",
    "        patience_counter = 0\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        \n",
    "    lr = optimizer.param_groups[0]['lr']\n",
    "    print(f'Epoch: {epoch:03d}, Loss: {loss:.4f}, Val MAE: {val_mae:.4f}, Best Val MAE: {best_val_mae:.4f}, LR: {lr:.6f}')\n",
    "        \n",
    "    if patience_counter >= patience:\n",
    "        print(f\"Early stopping at epoch {epoch}.\")\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5590365b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
