{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6ffd800d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Project\\Thermophysical Property Melting Point\\.venv\\Lib\\site-packages\\torch\\cuda\\__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.\n",
      "  import pynvml  # type: ignore[import]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pickle\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "from rdkit import Chem\n",
    "from rdkit import RDLogger \n",
    "RDLogger.DisableLog('rdApp.*')\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import Linear, BatchNorm1d, Module, Sequential\n",
    "from torch_geometric.data import Data, Dataset\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.nn import GCNConv, global_mean_pool\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f2cbb8b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Step 1: Defining Data Processing for Hybrid Model ---\n"
     ]
    }
   ],
   "source": [
    "print(\"--- Step 1: Defining Data Processing for Hybrid Model ---\")\n",
    "\n",
    "train_tabular = pd.read_csv('../data/processed/train_processed.csv')\n",
    "test_tabular = pd.read_csv('../data/processed/test_processed.csv')\n",
    "\n",
    "train_raw = pd.read_csv('../data/raw/train.csv')\n",
    "test_raw = pd.read_csv('../data/raw/test.csv')\n",
    "\n",
    "train_merged = pd.merge(train_raw[['id', 'SMILES']], train_tabular, on='id')\n",
    "test_merged = pd.merge(test_raw[['id', 'SMILES']], test_tabular, on='id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b1f338b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "rdkit_features = [col for col in train_tabular.columns if not col.startswith('Group_') and col not in ['id', 'Tm']]\n",
    "X_train_rdkit = train_merged[rdkit_features]\n",
    "X_test_rdkit = test_merged[rdkit_features]\n",
    "\n",
    "rdkit_scaler = StandardScaler()\n",
    "X_train_rdkit_scaled = rdkit_scaler.fit_transform(X_train_rdkit)\n",
    "X_test_rdkit_scaled = rdkit_scaler.transform(X_test_rdkit)\n",
    "\n",
    "with open('../models/rdkit_scaler.pkl', 'wb') as f:\n",
    "    pickle.dump(rdkit_scaler, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7c2ba21b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def smiles_to_graph(smiles_string, y_val=0, rdkit_feats=None):\n",
    "    mol = Chem.MolFromSmiles(smiles_string)\n",
    "    if mol is None: return None\n",
    "    atom_features_list = []\n",
    "    for atom in mol.GetAtoms():\n",
    "        atom_features_list.append([\n",
    "            atom.GetAtomicNum(), atom.GetFormalCharge(), atom.GetHybridization(),\n",
    "            atom.GetIsAromatic(), atom.GetTotalNumHs(), atom.GetTotalValence(),\n",
    "        ])\n",
    "    x = torch.tensor(atom_features_list, dtype=torch.float)\n",
    "    edge_indices, edge_attrs = [], []\n",
    "    for bond in mol.GetBonds():\n",
    "        i, j = bond.GetBeginAtomIdx(), bond.GetEndAtomIdx()\n",
    "        bond_type = bond.GetBondTypeAsDouble()\n",
    "        edge_indices.extend([(i, j), (j, i)])\n",
    "        edge_attrs.extend([[bond_type], [bond_type]])\n",
    "    edge_index = torch.tensor(edge_indices, dtype=torch.long).t().contiguous()\n",
    "    edge_attr = torch.tensor(edge_attrs, dtype=torch.float)\n",
    "    \n",
    "    data = Data(x=x, edge_index=edge_index, edge_attr=edge_attr, y=torch.tensor([y_val], dtype=torch.float))\n",
    "    if rdkit_feats is not None:\n",
    "        data.rdkit_features = torch.tensor([rdkit_feats], dtype=torch.float)\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "18c588cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HybridDataset(Dataset):\n",
    "    def __init__(self, root, data_df, rdkit_features_scaled, test=False, node_feature_scaler=None):\n",
    "        self.data_df = data_df\n",
    "        self.rdkit_features_scaled = rdkit_features_scaled\n",
    "        self.test = test\n",
    "        self.node_feature_scaler = node_feature_scaler\n",
    "        super(HybridDataset, self).__init__(root)\n",
    "        \n",
    "    @property\n",
    "    def raw_file_names(self): return []\n",
    "    @property\n",
    "    def processed_file_names(self): return [f'{\"test\" if self.test else \"train\"}_hybrid.pt']\n",
    "    def download(self): pass\n",
    "\n",
    "    def process(self):\n",
    "        if not self.test and self.node_feature_scaler is None:\n",
    "            all_node_features = []\n",
    "            for smiles in tqdm(self.data_df['SMILES'], desc=\"Fitting Node Scaler\"):\n",
    "                mol = Chem.MolFromSmiles(smiles)\n",
    "                if mol:\n",
    "                    for atom in mol.GetAtoms():\n",
    "                        all_node_features.append([atom.GetAtomicNum(), atom.GetFormalCharge(), atom.GetHybridization(), atom.GetIsAromatic(), atom.GetTotalNumHs(), atom.GetTotalValence()])\n",
    "            self.node_feature_scaler = StandardScaler()\n",
    "            self.node_feature_scaler.fit(all_node_features)\n",
    "            with open('../models/gnn_node_scaler.pkl', 'wb') as f:\n",
    "                pickle.dump(self.node_feature_scaler, f)\n",
    "\n",
    "        graphs = []\n",
    "        for idx, row in tqdm(self.data_df.iterrows(), total=self.data_df.shape[0], desc=\"Processing SMILES for Hybrid Model\"):\n",
    "            y_val = np.log(row['Tm']) if not self.test else 0\n",
    "            rdkit_feats = self.rdkit_features_scaled[idx]\n",
    "            graph = smiles_to_graph(row['SMILES'], y_val, rdkit_feats)\n",
    "            if graph is not None:\n",
    "                graph.x = torch.tensor(self.node_feature_scaler.transform(graph.x), dtype=torch.float)\n",
    "                graphs.append(graph)\n",
    "        torch.save(graphs, self.processed_paths[0], pickle_protocol=5)\n",
    "\n",
    "    def len(self):\n",
    "        if not hasattr(self, 'graphs'):\n",
    "            self.graphs = torch.load(self.processed_paths[0], weights_only=False)\n",
    "        return len(self.graphs)\n",
    "    def get(self, idx):\n",
    "        if not hasattr(self, 'graphs'):\n",
    "            self.graphs = torch.load(self.processed_paths[0], weights_only=False)\n",
    "        return self.graphs[idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0dd5e5d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instantiating Hybrid Datasets...\n",
      "Hybrid datasets created.\n"
     ]
    }
   ],
   "source": [
    "print(\"Instantiating Hybrid Datasets...\")\n",
    "train_dataset = HybridDataset(root='../data/processed/gnn_hybrid', data_df=train_merged, rdkit_features_scaled=X_train_rdkit_scaled)\n",
    "with open('../models/gnn_node_scaler.pkl', 'rb') as f:\n",
    "    node_scaler = pickle.load(f)\n",
    "test_dataset = HybridDataset(root='../data/processed/gnn_hybrid', data_df=test_merged, rdkit_features_scaled=X_test_rdkit_scaled, test=True, node_feature_scaler=node_scaler)\n",
    "print(\"Hybrid datasets created.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6d2914b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Step 2: Defining the Hybrid GNN-MLP Architecture ---\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- Step 2: Defining the Hybrid GNN-MLP Architecture ---\")\n",
    "\n",
    "class HybridGNN(torch.nn.Module):\n",
    "    def __init__(self, num_node_features, num_rdkit_features, hidden_channels=128):\n",
    "        super(HybridGNN, self).__init__()\n",
    "        torch.manual_seed(42)\n",
    "        self.conv1 = GCNConv(num_node_features, hidden_channels)\n",
    "        self.bn1 = BatchNorm1d(hidden_channels)\n",
    "        self.conv2 = GCNConv(hidden_channels, hidden_channels * 2)\n",
    "        self.bn2 = BatchNorm1d(hidden_channels * 2)\n",
    "        \n",
    "        self.mlp = Sequential(\n",
    "            Linear(hidden_channels * 2 + num_rdkit_features, hidden_channels * 2), \n",
    "            torch.nn.ReLU(),\n",
    "            BatchNorm1d(hidden_channels * 2),\n",
    "            Linear(hidden_channels * 2, hidden_channels),\n",
    "            torch.nn.ReLU(),\n",
    "            BatchNorm1d(hidden_channels),\n",
    "            Linear(hidden_channels, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index, batch, rdkit_feats = data.x, data.edge_index, data.batch, data.rdkit_features\n",
    "        x = self.conv1(x, edge_index).relu()\n",
    "        x = self.bn1(x)\n",
    "        x = self.conv2(x, edge_index).relu()\n",
    "        x = self.bn2(x)\n",
    "        graph_embedding = global_mean_pool(x, batch)\n",
    "        \n",
    "        combined_features = torch.cat([graph_embedding, rdkit_feats], dim=1)\n",
    "        \n",
    "        # Final prediction\n",
    "        return self.mlp(combined_features).squeeze()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9c0ffe1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Step 3: Starting Training for Hybrid Model ---\n"
     ]
    }
   ],
   "source": [
    "# 3. Training and Submission\n",
    "print(\"\\n--- Step 3: Starting Training for Hybrid Model ---\")\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "torch.manual_seed(42)\n",
    "shuffled_dataset = train_dataset.shuffle()\n",
    "train_size = int(0.85 * len(shuffled_dataset))\n",
    "train_data, val_data = shuffled_dataset[:train_size], shuffled_dataset[train_size:]\n",
    "train_loader = DataLoader(train_data, batch_size=64, shuffle=True)\n",
    "val_loader = DataLoader(val_data, batch_size=64, shuffle=False)\n",
    "\n",
    "model = HybridGNN(\n",
    "    num_node_features=train_dataset.num_node_features,\n",
    "    num_rdkit_features=X_train_rdkit_scaled.shape[1]\n",
    ").to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=5e-4)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.7, patience=10, min_lr=1e-6)\n",
    "criterion = torch.nn.L1Loss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1b35c67a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001, Loss: 1.6693, Val MAE: 4.1955, Best Val MAE: 4.1955, LR: 0.000500\n",
      "Epoch: 002, Loss: 1.4933, Val MAE: 3.5116, Best Val MAE: 3.5116, LR: 0.000500\n",
      "Epoch: 003, Loss: 1.2206, Val MAE: 3.2386, Best Val MAE: 3.2386, LR: 0.000500\n",
      "Epoch: 004, Loss: 0.8361, Val MAE: 2.7251, Best Val MAE: 2.7251, LR: 0.000500\n",
      "Epoch: 005, Loss: 0.4351, Val MAE: 1.6005, Best Val MAE: 1.6005, LR: 0.000500\n",
      "Epoch: 006, Loss: 0.1982, Val MAE: 0.9245, Best Val MAE: 0.9245, LR: 0.000500\n",
      "Epoch: 007, Loss: 0.1437, Val MAE: 0.5808, Best Val MAE: 0.5808, LR: 0.000500\n",
      "Epoch: 008, Loss: 0.0968, Val MAE: 0.5036, Best Val MAE: 0.5036, LR: 0.000500\n",
      "Epoch: 009, Loss: 0.0881, Val MAE: 0.3526, Best Val MAE: 0.3526, LR: 0.000500\n",
      "Epoch: 010, Loss: 0.0667, Val MAE: 0.3471, Best Val MAE: 0.3471, LR: 0.000500\n",
      "Epoch: 011, Loss: 0.0625, Val MAE: 0.3524, Best Val MAE: 0.3471, LR: 0.000500\n",
      "Epoch: 012, Loss: 0.0545, Val MAE: 0.3068, Best Val MAE: 0.3068, LR: 0.000500\n",
      "Epoch: 013, Loss: 0.0622, Val MAE: 0.4561, Best Val MAE: 0.3068, LR: 0.000500\n",
      "Epoch: 014, Loss: 0.0550, Val MAE: 0.3307, Best Val MAE: 0.3068, LR: 0.000500\n",
      "Epoch: 015, Loss: 0.0480, Val MAE: 0.2339, Best Val MAE: 0.2339, LR: 0.000500\n",
      "Epoch: 016, Loss: 0.0518, Val MAE: 0.2649, Best Val MAE: 0.2339, LR: 0.000500\n",
      "Epoch: 017, Loss: 0.0528, Val MAE: 0.3060, Best Val MAE: 0.2339, LR: 0.000500\n",
      "Epoch: 018, Loss: 0.0479, Val MAE: 0.2993, Best Val MAE: 0.2339, LR: 0.000500\n",
      "Epoch: 019, Loss: 0.0443, Val MAE: 0.2007, Best Val MAE: 0.2007, LR: 0.000500\n",
      "Epoch: 020, Loss: 0.0423, Val MAE: 0.2661, Best Val MAE: 0.2007, LR: 0.000500\n",
      "Epoch: 021, Loss: 0.0470, Val MAE: 0.2536, Best Val MAE: 0.2007, LR: 0.000500\n",
      "Epoch: 022, Loss: 0.0402, Val MAE: 0.2008, Best Val MAE: 0.2007, LR: 0.000500\n",
      "Epoch: 023, Loss: 0.0397, Val MAE: 0.3767, Best Val MAE: 0.2007, LR: 0.000500\n",
      "Epoch: 024, Loss: 0.0390, Val MAE: 0.3222, Best Val MAE: 0.2007, LR: 0.000500\n",
      "Epoch: 025, Loss: 0.0455, Val MAE: 0.2599, Best Val MAE: 0.2007, LR: 0.000500\n",
      "Epoch: 026, Loss: 0.0366, Val MAE: 0.1779, Best Val MAE: 0.1779, LR: 0.000500\n",
      "Epoch: 027, Loss: 0.0331, Val MAE: 0.2214, Best Val MAE: 0.1779, LR: 0.000500\n",
      "Epoch: 028, Loss: 0.0376, Val MAE: 0.2574, Best Val MAE: 0.1779, LR: 0.000500\n",
      "Epoch: 029, Loss: 0.0340, Val MAE: 0.1452, Best Val MAE: 0.1452, LR: 0.000500\n",
      "Epoch: 030, Loss: 0.0401, Val MAE: 0.2423, Best Val MAE: 0.1452, LR: 0.000500\n",
      "Epoch: 031, Loss: 0.0380, Val MAE: 0.1788, Best Val MAE: 0.1452, LR: 0.000500\n",
      "Epoch: 032, Loss: 0.0362, Val MAE: 0.2303, Best Val MAE: 0.1452, LR: 0.000500\n",
      "Epoch: 033, Loss: 0.0357, Val MAE: 0.2414, Best Val MAE: 0.1452, LR: 0.000500\n",
      "Epoch: 034, Loss: 0.0323, Val MAE: 0.1850, Best Val MAE: 0.1452, LR: 0.000500\n",
      "Epoch: 035, Loss: 0.0345, Val MAE: 0.1666, Best Val MAE: 0.1452, LR: 0.000500\n",
      "Epoch: 036, Loss: 0.0387, Val MAE: 0.2002, Best Val MAE: 0.1452, LR: 0.000500\n",
      "Epoch: 037, Loss: 0.0371, Val MAE: 0.2175, Best Val MAE: 0.1452, LR: 0.000500\n",
      "Epoch: 038, Loss: 0.0354, Val MAE: 0.1685, Best Val MAE: 0.1452, LR: 0.000500\n",
      "Epoch: 039, Loss: 0.0349, Val MAE: 0.1944, Best Val MAE: 0.1452, LR: 0.000500\n",
      "Epoch: 040, Loss: 0.0307, Val MAE: 0.1774, Best Val MAE: 0.1452, LR: 0.000350\n",
      "Epoch: 041, Loss: 0.0296, Val MAE: 0.2029, Best Val MAE: 0.1452, LR: 0.000350\n",
      "Epoch: 042, Loss: 0.0287, Val MAE: 0.1637, Best Val MAE: 0.1452, LR: 0.000350\n",
      "Epoch: 043, Loss: 0.0287, Val MAE: 0.1606, Best Val MAE: 0.1452, LR: 0.000350\n",
      "Epoch: 044, Loss: 0.0266, Val MAE: 0.1394, Best Val MAE: 0.1394, LR: 0.000350\n",
      "Epoch: 045, Loss: 0.0264, Val MAE: 0.1731, Best Val MAE: 0.1394, LR: 0.000350\n",
      "Epoch: 046, Loss: 0.0264, Val MAE: 0.1652, Best Val MAE: 0.1394, LR: 0.000350\n",
      "Epoch: 047, Loss: 0.0280, Val MAE: 0.1665, Best Val MAE: 0.1394, LR: 0.000350\n",
      "Epoch: 048, Loss: 0.0254, Val MAE: 0.1851, Best Val MAE: 0.1394, LR: 0.000350\n",
      "Epoch: 049, Loss: 0.0305, Val MAE: 0.1799, Best Val MAE: 0.1394, LR: 0.000350\n",
      "Epoch: 050, Loss: 0.0324, Val MAE: 0.1698, Best Val MAE: 0.1394, LR: 0.000350\n",
      "Epoch: 051, Loss: 0.0291, Val MAE: 0.1876, Best Val MAE: 0.1394, LR: 0.000350\n",
      "Epoch: 052, Loss: 0.0252, Val MAE: 0.1709, Best Val MAE: 0.1394, LR: 0.000350\n",
      "Epoch: 053, Loss: 0.0254, Val MAE: 0.1510, Best Val MAE: 0.1394, LR: 0.000350\n",
      "Epoch: 054, Loss: 0.0310, Val MAE: 0.2231, Best Val MAE: 0.1394, LR: 0.000350\n",
      "Epoch: 055, Loss: 0.0360, Val MAE: 0.1576, Best Val MAE: 0.1394, LR: 0.000245\n",
      "Epoch: 056, Loss: 0.0255, Val MAE: 0.1808, Best Val MAE: 0.1394, LR: 0.000245\n",
      "Epoch: 057, Loss: 0.0252, Val MAE: 0.1346, Best Val MAE: 0.1346, LR: 0.000245\n",
      "Epoch: 058, Loss: 0.0236, Val MAE: 0.1904, Best Val MAE: 0.1346, LR: 0.000245\n",
      "Epoch: 059, Loss: 0.0278, Val MAE: 0.1741, Best Val MAE: 0.1346, LR: 0.000245\n",
      "Epoch: 060, Loss: 0.0264, Val MAE: 0.1291, Best Val MAE: 0.1291, LR: 0.000245\n",
      "Epoch: 061, Loss: 0.0252, Val MAE: 0.1490, Best Val MAE: 0.1291, LR: 0.000245\n",
      "Epoch: 062, Loss: 0.0213, Val MAE: 0.1372, Best Val MAE: 0.1291, LR: 0.000245\n",
      "Epoch: 063, Loss: 0.0243, Val MAE: 0.1473, Best Val MAE: 0.1291, LR: 0.000245\n",
      "Epoch: 064, Loss: 0.0241, Val MAE: 0.1372, Best Val MAE: 0.1291, LR: 0.000245\n",
      "Epoch: 065, Loss: 0.0220, Val MAE: 0.1347, Best Val MAE: 0.1291, LR: 0.000245\n",
      "Epoch: 066, Loss: 0.0220, Val MAE: 0.1648, Best Val MAE: 0.1291, LR: 0.000245\n",
      "Epoch: 067, Loss: 0.0222, Val MAE: 0.1788, Best Val MAE: 0.1291, LR: 0.000245\n",
      "Epoch: 068, Loss: 0.0238, Val MAE: 0.1507, Best Val MAE: 0.1291, LR: 0.000245\n",
      "Epoch: 069, Loss: 0.0246, Val MAE: 0.1404, Best Val MAE: 0.1291, LR: 0.000245\n",
      "Epoch: 070, Loss: 0.0210, Val MAE: 0.1428, Best Val MAE: 0.1291, LR: 0.000245\n",
      "Epoch: 071, Loss: 0.0242, Val MAE: 0.1333, Best Val MAE: 0.1291, LR: 0.000171\n",
      "Epoch: 072, Loss: 0.0207, Val MAE: 0.1266, Best Val MAE: 0.1266, LR: 0.000171\n",
      "Epoch: 073, Loss: 0.0224, Val MAE: 0.1247, Best Val MAE: 0.1247, LR: 0.000171\n",
      "Epoch: 074, Loss: 0.0202, Val MAE: 0.1478, Best Val MAE: 0.1247, LR: 0.000171\n",
      "Epoch: 075, Loss: 0.0200, Val MAE: 0.1484, Best Val MAE: 0.1247, LR: 0.000171\n",
      "Epoch: 076, Loss: 0.0209, Val MAE: 0.1405, Best Val MAE: 0.1247, LR: 0.000171\n",
      "Epoch: 077, Loss: 0.0208, Val MAE: 0.1328, Best Val MAE: 0.1247, LR: 0.000171\n",
      "Epoch: 078, Loss: 0.0203, Val MAE: 0.1527, Best Val MAE: 0.1247, LR: 0.000171\n",
      "Epoch: 079, Loss: 0.0216, Val MAE: 0.1230, Best Val MAE: 0.1230, LR: 0.000171\n",
      "Epoch: 080, Loss: 0.0193, Val MAE: 0.1270, Best Val MAE: 0.1230, LR: 0.000171\n",
      "Epoch: 081, Loss: 0.0210, Val MAE: 0.1305, Best Val MAE: 0.1230, LR: 0.000171\n",
      "Epoch: 082, Loss: 0.0200, Val MAE: 0.1198, Best Val MAE: 0.1198, LR: 0.000171\n",
      "Epoch: 083, Loss: 0.0203, Val MAE: 0.1594, Best Val MAE: 0.1198, LR: 0.000171\n",
      "Epoch: 084, Loss: 0.0206, Val MAE: 0.1222, Best Val MAE: 0.1198, LR: 0.000171\n",
      "Epoch: 085, Loss: 0.0222, Val MAE: 0.1345, Best Val MAE: 0.1198, LR: 0.000171\n",
      "Epoch: 086, Loss: 0.0211, Val MAE: 0.1366, Best Val MAE: 0.1198, LR: 0.000171\n",
      "Epoch: 087, Loss: 0.0197, Val MAE: 0.1566, Best Val MAE: 0.1198, LR: 0.000171\n",
      "Epoch: 088, Loss: 0.0208, Val MAE: 0.1154, Best Val MAE: 0.1154, LR: 0.000171\n",
      "Epoch: 089, Loss: 0.0205, Val MAE: 0.1238, Best Val MAE: 0.1154, LR: 0.000171\n",
      "Epoch: 090, Loss: 0.0192, Val MAE: 0.1275, Best Val MAE: 0.1154, LR: 0.000171\n",
      "Epoch: 091, Loss: 0.0205, Val MAE: 0.1326, Best Val MAE: 0.1154, LR: 0.000171\n",
      "Epoch: 092, Loss: 0.0202, Val MAE: 0.1427, Best Val MAE: 0.1154, LR: 0.000171\n",
      "Epoch: 093, Loss: 0.0192, Val MAE: 0.1296, Best Val MAE: 0.1154, LR: 0.000171\n",
      "Epoch: 094, Loss: 0.0187, Val MAE: 0.1573, Best Val MAE: 0.1154, LR: 0.000171\n",
      "Epoch: 095, Loss: 0.0192, Val MAE: 0.1286, Best Val MAE: 0.1154, LR: 0.000171\n",
      "Epoch: 096, Loss: 0.0199, Val MAE: 0.1247, Best Val MAE: 0.1154, LR: 0.000171\n",
      "Epoch: 097, Loss: 0.0192, Val MAE: 0.1294, Best Val MAE: 0.1154, LR: 0.000171\n",
      "Epoch: 098, Loss: 0.0190, Val MAE: 0.1176, Best Val MAE: 0.1154, LR: 0.000171\n",
      "Epoch: 099, Loss: 0.0218, Val MAE: 0.1206, Best Val MAE: 0.1154, LR: 0.000120\n",
      "Epoch: 100, Loss: 0.0185, Val MAE: 0.1153, Best Val MAE: 0.1153, LR: 0.000120\n",
      "Epoch: 101, Loss: 0.0191, Val MAE: 0.1208, Best Val MAE: 0.1153, LR: 0.000120\n",
      "Epoch: 102, Loss: 0.0187, Val MAE: 0.1234, Best Val MAE: 0.1153, LR: 0.000120\n",
      "Epoch: 103, Loss: 0.0181, Val MAE: 0.1230, Best Val MAE: 0.1153, LR: 0.000120\n",
      "Epoch: 104, Loss: 0.0175, Val MAE: 0.1318, Best Val MAE: 0.1153, LR: 0.000120\n",
      "Epoch: 105, Loss: 0.0173, Val MAE: 0.1245, Best Val MAE: 0.1153, LR: 0.000120\n",
      "Epoch: 106, Loss: 0.0185, Val MAE: 0.1110, Best Val MAE: 0.1110, LR: 0.000120\n",
      "Epoch: 107, Loss: 0.0185, Val MAE: 0.1465, Best Val MAE: 0.1110, LR: 0.000120\n",
      "Epoch: 108, Loss: 0.0185, Val MAE: 0.1190, Best Val MAE: 0.1110, LR: 0.000120\n",
      "Epoch: 109, Loss: 0.0170, Val MAE: 0.1269, Best Val MAE: 0.1110, LR: 0.000120\n",
      "Epoch: 110, Loss: 0.0171, Val MAE: 0.1262, Best Val MAE: 0.1110, LR: 0.000120\n",
      "Epoch: 111, Loss: 0.0167, Val MAE: 0.1270, Best Val MAE: 0.1110, LR: 0.000120\n",
      "Epoch: 112, Loss: 0.0179, Val MAE: 0.1204, Best Val MAE: 0.1110, LR: 0.000120\n",
      "Epoch: 113, Loss: 0.0176, Val MAE: 0.1220, Best Val MAE: 0.1110, LR: 0.000120\n",
      "Epoch: 114, Loss: 0.0169, Val MAE: 0.1184, Best Val MAE: 0.1110, LR: 0.000120\n",
      "Epoch: 115, Loss: 0.0187, Val MAE: 0.1339, Best Val MAE: 0.1110, LR: 0.000120\n",
      "Epoch: 116, Loss: 0.0181, Val MAE: 0.1199, Best Val MAE: 0.1110, LR: 0.000120\n",
      "Epoch: 117, Loss: 0.0173, Val MAE: 0.1166, Best Val MAE: 0.1110, LR: 0.000084\n",
      "Epoch: 118, Loss: 0.0162, Val MAE: 0.1162, Best Val MAE: 0.1110, LR: 0.000084\n",
      "Epoch: 119, Loss: 0.0170, Val MAE: 0.1131, Best Val MAE: 0.1110, LR: 0.000084\n",
      "Epoch: 120, Loss: 0.0167, Val MAE: 0.1205, Best Val MAE: 0.1110, LR: 0.000084\n",
      "Epoch: 121, Loss: 0.0162, Val MAE: 0.1096, Best Val MAE: 0.1096, LR: 0.000084\n",
      "Epoch: 122, Loss: 0.0154, Val MAE: 0.1123, Best Val MAE: 0.1096, LR: 0.000084\n",
      "Epoch: 123, Loss: 0.0147, Val MAE: 0.1244, Best Val MAE: 0.1096, LR: 0.000084\n",
      "Epoch: 124, Loss: 0.0162, Val MAE: 0.1113, Best Val MAE: 0.1096, LR: 0.000084\n",
      "Epoch: 125, Loss: 0.0162, Val MAE: 0.1209, Best Val MAE: 0.1096, LR: 0.000084\n",
      "Epoch: 126, Loss: 0.0157, Val MAE: 0.1085, Best Val MAE: 0.1085, LR: 0.000084\n",
      "Epoch: 127, Loss: 0.0155, Val MAE: 0.1102, Best Val MAE: 0.1085, LR: 0.000084\n",
      "Epoch: 128, Loss: 0.0171, Val MAE: 0.1151, Best Val MAE: 0.1085, LR: 0.000084\n",
      "Epoch: 129, Loss: 0.0156, Val MAE: 0.1137, Best Val MAE: 0.1085, LR: 0.000084\n",
      "Epoch: 130, Loss: 0.0170, Val MAE: 0.1180, Best Val MAE: 0.1085, LR: 0.000084\n",
      "Epoch: 131, Loss: 0.0179, Val MAE: 0.1188, Best Val MAE: 0.1085, LR: 0.000084\n",
      "Epoch: 132, Loss: 0.0160, Val MAE: 0.1185, Best Val MAE: 0.1085, LR: 0.000084\n",
      "Epoch: 133, Loss: 0.0171, Val MAE: 0.1214, Best Val MAE: 0.1085, LR: 0.000084\n",
      "Epoch: 134, Loss: 0.0167, Val MAE: 0.1288, Best Val MAE: 0.1085, LR: 0.000084\n",
      "Epoch: 135, Loss: 0.0159, Val MAE: 0.1142, Best Val MAE: 0.1085, LR: 0.000084\n",
      "Epoch: 136, Loss: 0.0167, Val MAE: 0.1183, Best Val MAE: 0.1085, LR: 0.000084\n",
      "Epoch: 137, Loss: 0.0151, Val MAE: 0.1106, Best Val MAE: 0.1085, LR: 0.000059\n",
      "Epoch: 138, Loss: 0.0166, Val MAE: 0.1191, Best Val MAE: 0.1085, LR: 0.000059\n",
      "Epoch: 139, Loss: 0.0148, Val MAE: 0.1166, Best Val MAE: 0.1085, LR: 0.000059\n",
      "Epoch: 140, Loss: 0.0144, Val MAE: 0.1132, Best Val MAE: 0.1085, LR: 0.000059\n",
      "Epoch: 141, Loss: 0.0141, Val MAE: 0.1160, Best Val MAE: 0.1085, LR: 0.000059\n",
      "Epoch: 142, Loss: 0.0155, Val MAE: 0.1175, Best Val MAE: 0.1085, LR: 0.000059\n",
      "Epoch: 143, Loss: 0.0146, Val MAE: 0.1111, Best Val MAE: 0.1085, LR: 0.000059\n",
      "Epoch: 144, Loss: 0.0148, Val MAE: 0.1132, Best Val MAE: 0.1085, LR: 0.000059\n",
      "Epoch: 145, Loss: 0.0151, Val MAE: 0.1093, Best Val MAE: 0.1085, LR: 0.000059\n",
      "Epoch: 146, Loss: 0.0142, Val MAE: 0.1086, Best Val MAE: 0.1085, LR: 0.000059\n",
      "Epoch: 147, Loss: 0.0144, Val MAE: 0.1125, Best Val MAE: 0.1085, LR: 0.000059\n",
      "Epoch: 148, Loss: 0.0144, Val MAE: 0.1124, Best Val MAE: 0.1085, LR: 0.000041\n",
      "Epoch: 149, Loss: 0.0137, Val MAE: 0.1114, Best Val MAE: 0.1085, LR: 0.000041\n",
      "Epoch: 150, Loss: 0.0144, Val MAE: 0.1121, Best Val MAE: 0.1085, LR: 0.000041\n",
      "Epoch: 151, Loss: 0.0144, Val MAE: 0.1108, Best Val MAE: 0.1085, LR: 0.000041\n",
      "Early stopping at epoch 151.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "def train():\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for data in train_loader:\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        out = model(data)\n",
    "        loss = criterion(out, data.y)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item() * data.num_graphs\n",
    "    return total_loss / len(train_loader.dataset)\n",
    "\n",
    "def evaluate(loader):\n",
    "    \"\"\"Evaluates the model on a given data loader.\"\"\"\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_reals = []\n",
    "    with torch.no_grad():\n",
    "        for data in loader:\n",
    "            data = data.to(device)\n",
    "            out = model(data)\n",
    "            preds = np.exp(out.cpu().numpy())\n",
    "            reals = np.exp(data.y.cpu().numpy())\n",
    "            all_preds.extend(preds)\n",
    "            all_reals.extend(reals)\n",
    "            \n",
    "    return mean_absolute_error(all_reals, all_preds)\n",
    "\n",
    "\n",
    "\n",
    "best_val_mae = float('inf')\n",
    "patience_counter = 0\n",
    "patience = 25\n",
    "\n",
    "for epoch in range(1, 201):\n",
    "    loss = train()\n",
    "    val_mae = evaluate(val_loader)\n",
    "    scheduler.step(val_mae)\n",
    "    \n",
    "    if val_mae < best_val_mae:\n",
    "        best_val_mae = val_mae\n",
    "        torch.save(model.state_dict(), 'best_hybrid_model.pt')\n",
    "        patience_counter = 0\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        \n",
    "    lr = optimizer.param_groups[0]['lr']\n",
    "    print(f'Epoch: {epoch:03d}, Loss: {loss:.4f}, Val MAE: {val_mae:.4f}, Best Val MAE: {best_val_mae:.4f}, LR: {lr:.6f}')\n",
    "        \n",
    "    if patience_counter >= patience:\n",
    "        print(f\"Early stopping at epoch {epoch}.\")\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "725b447e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Generating Final Submission from Hybrid Model ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f6379b4bcc94e6a86a8841011344298",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting with Hybrid Model:   0%|          | 0/11 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submission file 'submission_hybrid_gnn.csv' created successfully!\n",
      "     id        Tm\n",
      "0  1022  5.793273\n",
      "1  1146  5.745530\n",
      "2    79  5.348372\n",
      "3  2279  5.332273\n",
      "4  1342  5.318476\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- Generating Final Submission from Hybrid Model ---\")\n",
    "model.load_state_dict(torch.load('best_hybrid_model.pt'))\n",
    "test_preds = []\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for data in tqdm(test_loader, desc=\"Predicting with Hybrid Model\"):\n",
    "        data = data.to(device)\n",
    "        out = model(data)\n",
    "        test_preds.extend(np.exp(out.cpu().numpy()))\n",
    "\n",
    "submission_df = pd.read_csv('../data/raw/sample_submission.csv')\n",
    "submission_df['Tm'] = test_preds\n",
    "submission_df.to_csv('submission_hybrid_gnn.csv', index=False)\n",
    "print(\"Submission file 'submission_hybrid_gnn.csv' created successfully!\")\n",
    "print(submission_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0314b4bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating in Kelvin...\n",
      "\n",
      " Final Calibrated Result (Hybrid GNN): 30.2785 Kelvin\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "import torch\n",
    "\n",
    "def evaluate_in_kelvin(loader, model, device):\n",
    "    \"\"\"\n",
    "    Evaluates the model and converts predictions back to original Kelvin scale.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    all_preds_kelvin = []\n",
    "    all_reals_kelvin = []\n",
    "    \n",
    "    print(\"Evaluating in Kelvin...\")\n",
    "    with torch.no_grad():\n",
    "        for data in loader:\n",
    "            data = data.to(device)\n",
    "            out = model(data)\n",
    "            \n",
    "            preds_log1p = np.exp(out.cpu().numpy())\n",
    "            reals_log1p = np.exp(data.y.cpu().numpy())\n",
    "            \n",
    "            # Inverse the Feature Engineering log1p transformation (get back to Kelvin)\n",
    "            preds_kelvin = np.expm1(preds_log1p)\n",
    "            reals_kelvin = np.expm1(reals_log1p)\n",
    "            \n",
    "            all_preds_kelvin.extend(preds_kelvin)\n",
    "            all_reals_kelvin.extend(reals_kelvin)\n",
    "            \n",
    "    # Calculate MAE on the true Kelvin scale\n",
    "    mae = mean_absolute_error(all_reals_kelvin, all_preds_kelvin)\n",
    "    return mae, all_preds_kelvin\n",
    "\n",
    "# Calculate and print the true MAE\n",
    "true_mae, _ = evaluate_in_kelvin(val_loader, model, device)\n",
    "print(f\"\\n Final Calibrated Result (Hybrid GNN): {true_mae:.4f} Kelvin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac0aea2c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
